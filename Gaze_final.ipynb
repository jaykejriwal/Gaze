{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First step\n",
    "## Download the Gaze dataset to a specific location\n",
    "## Later, execute the code in sequence \n",
    "## This program reads each baseline (BL) file in the dataset and removes them from the folder.\n",
    "## Program to remove list of files with BL in the gaze data\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "#Specify path of the dataset\n",
    "fileList = glob.glob(r'D:\\Jay\\GAZE\\dataset\\Audio_Textgrid\\**\\*.*', recursive=True)\n",
    "\n",
    "for _file in fileList:\n",
    "    if re.search('-BL-', _file):\n",
    "        os.remove(_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Second step\n",
    "## The program renames all text grid and wav files by removing empty spaces between them\n",
    "# Rename list of files in the gaze dataset\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "#Specify path of the dataset\n",
    "fileList = glob.glob(r'D:\\Jay\\GAZE\\dataset\\Audio_Textgrid\\**\\*.*', recursive=True)\n",
    "\n",
    "for _file in fileList:\n",
    "    if re.search('\\s', _file):\n",
    "        new_name = _file.split(' ')[0]+'.TextGrid'\n",
    "        os.rename(_file, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third step \n",
    "## The program reads each textgrid file in the dataset folder and returns a text file with information about start, end and speaker\n",
    "## For instance, GA-CO-AMO.TextGrid will be read and GA-CO-AMO.txt will be generated in same folder\n",
    "\n",
    "# Program extracts start, stop and tier information from textgrid files\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import re\n",
    "import textgrid\n",
    "import numpy as np\n",
    "# To install textgrid package\n",
    "# git clone https://github.com/kylerbrown/textgrid.git\n",
    "# cd textgrid\n",
    "# pip install .\n",
    "\n",
    "#Specify path of the dataset\n",
    "list_of_files = glob.glob(r'D:\\Jay\\GAZE\\dataset\\Audio_Textgrid\\**\\*.TextGrid',recursive=True)\n",
    "\n",
    "for file_name in list_of_files:    \n",
    "    out_name=file_name[:-9]+'.txt'\n",
    "    tgrid = textgrid.read_textgrid(file_name)\n",
    "    df1=pd.DataFrame(tgrid)\n",
    "    df1['name'].replace('', np.nan, inplace=True)\n",
    "    df1.dropna(subset=['name'], inplace=True)\n",
    "    df1 = df1[df1[\"name\"].str.contains(\"conversation\") == False]\n",
    "    df1.drop('name', axis=1, inplace=True)\n",
    "    df1.sort_values(by=['start'],inplace=True)\n",
    "    df1.to_csv(out_name, index=False,header=False,sep='\\t')\n",
    "    del tgrid, df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fourth step \n",
    "## The program converts each wav file with sample rate of 8k into wav file with 16k format\n",
    "\n",
    "import subprocess\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#Specify input path of wav files\n",
    "list_of_wav_files = sorted(glob.glob(r'D:\\Jay\\GAZE\\dataset\\Audio_Textgrid\\**\\*.wav',recursive=True))\n",
    "\n",
    "#Specify output path of wav files\n",
    "new_path_resampled = r'D:\\Jay\\GAZE\\dataset\\Audio_resampled'\n",
    "def resample_wavfiles(infiles):\n",
    "    for i in range(0, len(infiles)):\n",
    "        current_file=os.path.basename(infiles[i])\n",
    "        print(\"processing file\", current_file)\n",
    "        outfile2 = os.path.join(new_path_resampled, current_file)\n",
    "\n",
    "        ffp=r\"C:\\ffmpeg-master-latest-win64-gpl\\bin\\ffmpeg\"\n",
    "        cmd2wav2 = ffp+' -i ' + infiles[i] + ' ' + \"-ac 1\" + ' ' + \"-ar 16000\" + ' ' + outfile2\n",
    "        print(cmd2wav2)\n",
    "        subprocess.call(cmd2wav2, shell=True)\n",
    "\n",
    "resample_wavfiles(list_of_wav_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fifth step \n",
    "## Program to extract utterance (text), semantic and auditory embeddings from each text file\n",
    "## The program read text file and extract embeddings from corresponding wav file\n",
    "## The program reads each text file generated in the third step and extracts TRILL vectors, semantic and textual embeddings from each turn\n",
    "## The output of embeddings is saved in pkl file format\n",
    "\n",
    "#Get utterance from text file and extract DNN embeddings \n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from functools import reduce\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import wave\n",
    "import tensorflow as tf1\n",
    "import tensorflow_hub as hub\n",
    "# Import TF 2.X and make sure we're running eager.\n",
    "import tensorflow.compat.v2 as tf\n",
    "#import tensorflow as tf\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "modulev3=None\n",
    "modulev3_graph=None\n",
    "import pickle\n",
    "tf.enable_v2_behavior()\n",
    "assert tf.executing_eagerly()\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "\n",
    "##Function to load TRILL vector model   \n",
    "def get_TRILLv3_signal(signal,samplerate):\n",
    "    global modulev3\n",
    "    if modulev3==None:\n",
    "        #Specify the path of TRILL vector model\n",
    "        print('******************\\nLoading model ...\\n******************')    \n",
    "        modulev3 = hub.load(r'D:\\Jay\\columbia-games-corpus\\trill_extraction_v2\\v3')\n",
    "    \n",
    "    \n",
    "    max_int16 = 2**15\n",
    "    chunks_cnt=int(signal.shape[0]/(samplerate*10.0))#10 seconds max in chunk\n",
    "    if chunks_cnt==0:\n",
    "        chunks=[signal]\n",
    "    else:\n",
    "        chunks=np.array_split(signal, chunks_cnt)\n",
    "    \n",
    "    trillv3_emb_all=np.empty(shape=(0,512))\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        trillv3 = modulev3(samples=chunk, sample_rate=samplerate)\n",
    "        trillv3_emb = trillv3['embedding']\n",
    "        trillv3_emb_all=np.concatenate((trillv3_emb_all, trillv3_emb))\n",
    "\n",
    "    trillv3_emb_avg = np.mean(trillv3_emb_all, axis=0, keepdims=False)\n",
    "\n",
    "    return (trillv3_emb_avg.tolist())    \n",
    "\n",
    "##Function to check if audio files are in proper format \n",
    "def check_wav_format(wav_file, start, end):\n",
    "    wf = wave.open(wav_file)\n",
    "    nchannels, sampwidth, framerate, nframes, comptype, compname = wf.getparams()\n",
    "    print(nchannels, sampwidth, framerate, nframes, comptype, compname)\n",
    "    wav_length = float(nframes) / float(framerate)\n",
    "    print(wav_length)\n",
    "    if nchannels!=1:\n",
    "        print('Error: Incoming audio file has more then 1 channel')\n",
    "        return(-1) \n",
    "\t\t\t\t\n",
    "    if framerate!=16000:\n",
    "        print('Error: Incoming audio file sampling frequency must be 16000')\n",
    "        return(-2)\n",
    "\t\t\t\t\n",
    "    if sampwidth!=2:\n",
    "        print('Error: Incoming audio file sample width must be 16 bit')\n",
    "        return(-3)\n",
    "\n",
    "    if wav_length<end:\n",
    "        print('Error: The duration of the audio file is shorter than the required end time')\n",
    "        return(-4)\n",
    "\t\t\n",
    "    if start<0.0:\n",
    "        print('Error: start time is lower then 0.0')\n",
    "        return(-5)\n",
    "    \n",
    "    if start>=end:\n",
    "        print('Error: start time is larger then end time')\n",
    "        return(-6)    \n",
    "\t\t\n",
    "\t\t\t\t\n",
    "    return(framerate)\n",
    "\n",
    "##Function to extract TRILL embeddings from specific file name with start and end time as parameters   \n",
    "##The function also uses facebook model to extract utterance from audio file i.e., speech to text\n",
    "def get_utterance_audiofile_from_to(wav_file,start,end):\n",
    "    print('get_signal:',wav_file,start,end)\n",
    "    samplerate=check_wav_format(wav_file, start, end)\n",
    "    if samplerate<0:\n",
    "        return(null)    \n",
    "    startsample=int(start*samplerate)\n",
    "    endsample=int(end*samplerate)\n",
    "    signal, samplerate = sf.read(wav_file,start=startsample, stop=endsample)\n",
    "    print(len(signal),samplerate)    \n",
    "    tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\",do_lower_case=True)\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    input_values = tokenizer(signal, return_tensors = \"pt\").input_values\n",
    "    logits = model(input_values).logits\n",
    "    prediction = torch.argmax(logits, dim = -1)\n",
    "    transcription = tokenizer.batch_decode(prediction)[0]\n",
    "    trill=get_TRILLv3_signal(signal,samplerate)\n",
    "    return(transcription,trill)\n",
    "\n",
    "#Specify path of dataset\n",
    "path = 'D:\\\\Jay\\\\GAZE\\\\dataset\\\\'\n",
    "\n",
    "#Specify path of audio files generated in previous step\n",
    "audio_path=r'D:\\Jay\\GAZE\\dataset\\Audio_resampled'\n",
    "\n",
    "#Specify path of text files where data will be saved\n",
    "text_path=r'D:\\Jay\\GAZE\\dataset\\Text'\n",
    "\n",
    "#Specify path of embeddings where data will be saved in pkl file format\n",
    "#Specify path where auditory embeddings needs to be saved\n",
    "audio_embedding_path = r'D:\\Jay\\GAZE\\Pickle\\audio'\n",
    "\n",
    "#Specify path where utterance embeddings needs to be saved\n",
    "text_embedding_path = r'D:\\Jay\\GAZE\\Pickle\\text'\n",
    "\n",
    "#Specify path where semantic embeddings needs to be saved\n",
    "semantic_embedding_path = r'D:\\Jay\\GAZE\\Pickle\\semantic'\n",
    "\n",
    "#Initialize transformer model for semantic feature extraction\n",
    "print('Loading Transformer...')\n",
    "model = SentenceTransformer(\"sentence-transformers/msmarco-distilbert-base-v4\")\n",
    "\n",
    "all_files = os.listdir(path)\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            current_file=os.path.join(root, file)\n",
    "            with open(current_file, 'r', encoding=\"utf8\") as f:\n",
    "                out_text_file= os.path.join(text_path, os.path.basename(file))\n",
    "                out_audio_embedding= os.path.join(audio_embedding_path, os.path.basename(file)).split('.')[-2]\n",
    "                out_text_embedding= os.path.join(text_embedding_path, os.path.basename(file)).split('.')[-2]\n",
    "                out_semantic_embedding= os.path.join(semantic_embedding_path, os.path.basename(file)).split('.')[-2]\n",
    "                sentence=[]\n",
    "                audioembeddings = []\n",
    "                sentence_embeddings_tensor = []\n",
    "                text = f.readlines()\n",
    "                for line in text:\n",
    "                    n1=line.split('\\t')[0]\n",
    "                    n2=line.split('\\t')[1]\n",
    "                    n3=os.path.join(audio_path, os.path.basename(file)).split('.')[-2]+'.wav'\n",
    "                    x,y=get_utterance_audiofile_from_to(n3,float(n1),float(n2))\n",
    "                    sentence.append(x)\n",
    "                    audioembeddings.append(y)\n",
    "                sentence_embeddings_tensor = model.encode(sentence, convert_to_tensor=False)\n",
    "                sentence_embeddings_tensor=sentence_embeddings_tensor.tolist()\n",
    " \n",
    "                with open(out_audio_embedding+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(audioembeddings, f)\n",
    "                with open(out_semantic_embedding+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(sentence_embeddings_tensor, f)\n",
    "                with open(out_text_embedding+'.pkl', 'wb') as f:\n",
    "                    pickle.dump(sentence, f)\n",
    "    \n",
    "                df1=pd.read_csv(current_file,delimiter='\\t',header=None, names=['start','end','Speaker'])\n",
    "                df1['Utterance'] = sentence\n",
    "                df1.insert(0, 'Turn_number', range(1, 1 + len(df1)))\n",
    "                df1.to_csv(out_text_file, index=False,header=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sixth step \n",
    "## Program to extract seven acoustic prosodic features using PRAAT toolkit\n",
    "## The program read text file and extract prosodic features from corresponding wav file\n",
    "## The program reads each text file generated in the fourth step and extracts pitch mean and max, inetnsity mean and max, Jitter, Shimmer, NHR from each turn \n",
    "## The output is saved in text file format\n",
    "\n",
    "#Extract acoustic prosodic features\n",
    "import pandas as pd\n",
    "import csv\n",
    "import subprocess\n",
    "import re\n",
    "import glob\n",
    "import os,sys\n",
    "import numpy as np\n",
    "\n",
    "#Specify path of the text files generated in previous step \n",
    "path = 'D:\\\\Jay\\\\GAZE\\\\dataset\\\\Text'\n",
    "\n",
    "#Specify path of audio files generated in fourth step\n",
    "audio_path=r'D:\\Jay\\GAZE\\dataset\\Audio_resampled'\n",
    "\n",
    "#Specify output folder\n",
    "output_path=r'D:\\Jay\\GAZE\\dataset\\Audio_ap'\n",
    "\n",
    "#Specify PRAAT path\n",
    "praat = 'D:\\\\D drive\\\\Praat.exe'\n",
    "\n",
    "#Specify PRAAT script path\n",
    "script= 'D:\\\\Jay\\\\GAZE\\\\dataset\\\\Praat script\\\\pitch,jitter,shimmer,intensity_new.praat'\n",
    "\n",
    "\n",
    "all_files = os.listdir(path)\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            current_file=os.path.join(root, file)\n",
    "            with open(current_file, 'r', encoding=\"utf8\") as f:\n",
    "                text = f.readlines()\n",
    "                output_filename = os.path.join(output_path, os.path.basename(file))\n",
    "                for line in text:\n",
    "                    n1=line.split('\\t')[1]\n",
    "                    n2=line.split('\\t')[2]\n",
    "                    n3=os.path.join(audio_path, os.path.basename(file)).split('.')[-2]+'.wav'\n",
    "                    print(n3)\n",
    "                    subprocess.call([praat, '--run', script, n3,n1,n2, output_filename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seventh step \n",
    "## Program to merge text transcripts\n",
    "## The program read all text files and merge them into one big file\n",
    "## The program reads each text file generated in the fourth step and extracts pitch mean and max, inetnsity mean and max, Jitter, Shimmer, NHR from each turn \n",
    "## One output file is generated and is saved in text file format\n",
    "## The output file has 8 columns namely Turn_number, start,\tstop, tier,\tFile_name, Condition, Participant_id, Utterance \n",
    "\n",
    "#Merge Text transcript\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#Specify path for input files generated in Fifth step\n",
    "list_of_files = glob.glob(r'D:\\Jay\\GAZE\\dataset\\Text\\*',recursive=True)\n",
    "\n",
    "#Specify path for output file \n",
    "out_name = r'D:\\Jay\\GAZE\\dataset\\merged_text_transcript'\n",
    "li = []\n",
    "\n",
    "for filename in list_of_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0, delimiter='\\t')\n",
    "    basename = os.path.basename(filename)\n",
    "    file_name = os.path.splitext(basename)[0]\n",
    "    df['File_name'] = file_name\n",
    "    df['Condition'] = df['File_name'].str.slice(0,2)\n",
    "    df['Participant'] = df['File_name'].str.slice(6,9)\n",
    "    #Re-arrange columns with filename as first column\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    #Define new arranged column to data-frame\n",
    "    df = df[cols]\n",
    "    #Append the dataframe to list\n",
    "    li.append(df)\n",
    "frame = pd.concat(li, axis=0, ignore_index=True) \n",
    "\n",
    "frame.to_csv(out_name+'.csv', index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eight step \n",
    "## Program to merge acoustic-prosodic files generated in sixth step\n",
    "## The program read all text files and merge them into one big file\n",
    "## The program reads each text file generated in the sixth step and combines them and save it in text file format\n",
    "## The output file has 12 columns namely \n",
    "# filename, duration, mean_pitch, min_pitch, \n",
    "# max_pitch, median_pitch, mean_intensity, min_intensity, \n",
    "# max_intensity, jitter_local, shimmer_local, mean_nhr\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#Specify path of acoustic-prosodic features file generated in sixth step\n",
    "list_of_files = glob.glob(r'D:\\Jay\\GAZE\\dataset\\Audio_ap\\*',recursive=True)\n",
    "\n",
    "#Specify path of output\n",
    "out_name = r'D:\\Jay\\GAZE\\dataset\\merged_audio_ap'\n",
    "li = []\n",
    "\n",
    "for filename in list_of_files:\n",
    "    df = pd.read_csv(filename, delimiter='\\t',header=None, names=['filename','Full_path','duration','mean_pitch','min_pitch',\n",
    "    'max_pitch','mean_intensity','min_intensity','max_intensity','jitter_local','shimmer_local','mean_nhr'])\n",
    "    basename = os.path.basename(filename)\n",
    "    file_name = os.path.splitext(basename)[0]\n",
    "    df['filename'] = file_name\n",
    "    df.drop(['Full_path'], axis=1,inplace=True)\n",
    "    li.append(df)\n",
    "frame = pd.concat(li, axis=0, ignore_index=True) \n",
    "\n",
    "frame.to_csv(out_name+'.csv', index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ninth step\n",
    "##Using excel, merge files manually generated in seventh and eighth step\n",
    "##Save the merged file and provide it as an input to current program\n",
    "##This program measures speech rate and perform z-score normalization\n",
    "#Measure speech rate and perform z-score normalization\n",
    "\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "def syllable_count_english(word):\n",
    "    count = 0\n",
    "    vowels = \"aeiouy\"\n",
    "\n",
    "    for word in word.lower().split(\" \"):\n",
    "        for i in range(len(word)):\n",
    "            if word[i] in vowels and (i == 0 or word[i-1] not in vowels):\n",
    "                count +=1\n",
    "    return count\n",
    "\n",
    "#Provide path for merged file as input\n",
    "ap_filename=r'D:\\Jay\\GAZE\\dataset\\Merged.csv'\n",
    "\n",
    "#Provide path for output file\n",
    "output=r'D:\\Jay\\GAZE\\dataset\\final_merge.csv'\n",
    "\n",
    "\n",
    "with open(ap_filename,encoding='utf-8') as csv_file:\n",
    "    df = pd.read_csv(csv_file)\n",
    "df.drop(['File_name'], axis=1,inplace=True)\n",
    "df.replace('--undefined--', 0,inplace=True)\n",
    "df['Speaker'] = np.where(df['Speaker'] == 'participant', df['Participant'], df['Speaker'])\n",
    "df.Utterance=df.Utterance.astype(str)\n",
    "df['speechrate'] = df['Utterance'].map(syllable_count_english)/(df['duration'])\n",
    "df['mean_pitch']=df['mean_pitch'].astype(np.float)\n",
    "df['min_pitch']=df['min_pitch'].astype(np.float)\n",
    "df['max_pitch']=df['max_pitch'].astype(np.float)\n",
    "df['mean_intensity']=df['mean_intensity'].astype(np.float)\n",
    "df['min_intensity']=df['min_intensity'].astype(np.float)\n",
    "df['max_intensity']=df['max_intensity'].astype(np.float)\n",
    "df['jitter_local']=df['jitter_local'].astype(np.float)\n",
    "df['shimmer_local']=df['shimmer_local'].astype(np.float)\n",
    "df['mean_nhr']=df['mean_nhr'].astype(np.float)\n",
    "df['speechrate']=df['speechrate'].astype(np.float)\n",
    "#Perform z score normalization\n",
    "df['z_mean_pitch'] = df.groupby(['Speaker']).mean_pitch.transform(zscore, ddof=1)\n",
    "df['z_min_pitch'] = df.groupby(['Speaker']).min_pitch.transform(zscore, ddof=1)\n",
    "df['z_max_pitch'] = df.groupby(['Speaker']).max_pitch.transform(zscore, ddof=1)\n",
    "df['z_mean_intensity'] = df.groupby(['Speaker']).mean_intensity.transform(zscore, ddof=1)\n",
    "df['z_min_intensity'] = df.groupby(['Speaker']).min_intensity.transform(zscore, ddof=1)\n",
    "df['z_max_intensity'] = df.groupby(['Speaker']).max_intensity.transform(zscore, ddof=1)\n",
    "df['z_jitter_local'] = df.groupby(['Speaker']).jitter_local.transform(zscore, ddof=1)\n",
    "df['z_shimmer_local'] = df.groupby(['Speaker']).shimmer_local.transform(zscore, ddof=1)\n",
    "df['z_mean_nhr'] = df.groupby(['Speaker']).mean_nhr.transform(zscore, ddof=1)\n",
    "df['z_speechrate'] = df.groupby(['Speaker']).speechrate.transform(zscore, ddof=1)\n",
    "df.to_csv(output, index=False,sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tenth step\n",
    "##The program measures absolute distance on two adjacent turns on 8 acoustic-prosodic features.\n",
    "##The program reads file generated in previous step and measures L1 distance on each prosodic feature seperately\n",
    "##The file will have 14 columns namely\n",
    "#Turn_number, Participant, Condition, File_name, Mean Pitch distance, Min Pitch distance\t\n",
    "#Max Pitch distance, Mean Intensity distance, Min Intensity distance,Max Intensity distance,\t\n",
    "#Jitter distance, Shimmer distance, Mean Nhr distance and Speechrate distance\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "#Specify input \n",
    "file_name=r'D:\\Jay\\GAZE\\dataset\\final_merge.csv'\n",
    "\n",
    "#Specify output\n",
    "output_filename=r'D:\\Jay\\GAZE\\dataset\\final_ap_pairdistance_abs.csv'\n",
    "\n",
    "speaker=[]\n",
    "adjacent_file_name=[]\n",
    "adjacent_distance_mean_pitch=[]\n",
    "adjacent_distance_min_pitch=[]\n",
    "adjacent_distance_max_pitch=[]\n",
    "adjacent_distance_mean_intensity=[]\n",
    "adjacent_distance_min_intensity=[]\n",
    "adjacent_distance_max_intensity=[]\n",
    "adjacent_distance_jitter=[]\n",
    "adjacent_distance_shimmer=[]\n",
    "adjacent_distance_nhr=[]\n",
    "adjacent_distance_speechrate=[]\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_name,delimiter=',')\n",
    "\n",
    "df=df.applymap(str)\n",
    "for i in range(len(df)-1):\n",
    "    if(df.loc[i, \"filename\"]==df.loc[i+1, \"filename\"] and df.loc[i, \"Speaker\"]!=df.loc[i+1, \"Speaker\"]):\n",
    "        \n",
    "        df['z_mean_pitch']=df['z_mean_pitch'].astype(np.float)\n",
    "        df['z_min_pitch']=df['z_min_pitch'].astype(np.float)\n",
    "        df['z_max_pitch']=df['z_max_pitch'].astype(np.float)\n",
    "        df['z_mean_intensity']=df['z_mean_intensity'].astype(np.float)\n",
    "        df['z_min_intensity']=df['z_min_intensity'].astype(np.float)\n",
    "        df['z_max_intensity']=df['z_max_intensity'].astype(np.float)\n",
    "        df['z_jitter_local']=df['z_jitter_local'].astype(np.float)\n",
    "        df['z_shimmer_local']=df['z_shimmer_local'].astype(np.float)\n",
    "        df['z_mean_nhr']=df['z_mean_nhr'].astype(np.float)\n",
    "        df['z_speechrate']=df['z_speechrate'].astype(np.float)\n",
    "\n",
    "        file_name=(df.loc[i, \"filename\"])\n",
    "        adjacent_file_name.append(file_name)\n",
    "        pitch_mean_diff=(df.loc[i, \"z_mean_pitch\"]-df.loc[i+1, \"z_mean_pitch\"]).__abs__()\n",
    "        adjacent_distance_mean_pitch.append(pitch_mean_diff)\n",
    "        pitch_min_diff=(df.loc[i, \"z_min_pitch\"]-df.loc[i+1, \"z_min_pitch\"]).__abs__()\n",
    "        adjacent_distance_min_pitch.append(pitch_min_diff)\n",
    "        pitch_max_diff=(df.loc[i, \"z_max_pitch\"]-df.loc[i+1, \"z_max_pitch\"]).__abs__()\n",
    "        adjacent_distance_max_pitch.append(pitch_max_diff)\n",
    "        intensity_mean_diff=(df.loc[i, \"z_mean_intensity\"]-df.loc[i+1, \"z_mean_intensity\"]).__abs__()\n",
    "        adjacent_distance_mean_intensity.append(intensity_mean_diff)\n",
    "        intensity_min_diff=(df.loc[i, \"z_min_intensity\"]-df.loc[i+1, \"z_min_intensity\"]).__abs__()\n",
    "        adjacent_distance_min_intensity.append(intensity_min_diff)\n",
    "        intensity_max_diff=(df.loc[i, \"z_max_intensity\"]-df.loc[i+1, \"z_max_intensity\"]).__abs__()\n",
    "        adjacent_distance_max_intensity.append(intensity_max_diff)\n",
    "        jitter_diff=(df.loc[i, \"z_jitter_local\"]-df.loc[i+1, \"z_jitter_local\"]).__abs__()\n",
    "        adjacent_distance_jitter.append(jitter_diff)\n",
    "        shimmer_diff=(df.loc[i, \"z_shimmer_local\"]-df.loc[i+1, \"z_shimmer_local\"]).__abs__()\n",
    "        adjacent_distance_shimmer.append(shimmer_diff)\n",
    "        nhr_diff=(df.loc[i, \"z_mean_nhr\"]-df.loc[i+1, \"z_mean_nhr\"]).__abs__()\n",
    "        adjacent_distance_nhr.append(nhr_diff)\n",
    "        speechrate_diff=(df.loc[i, \"z_speechrate\"]-df.loc[i+1, \"z_speechrate\"]).__abs__()\n",
    "        adjacent_distance_speechrate.append(speechrate_diff)\n",
    "        \n",
    "        \n",
    "df1 = pd.DataFrame(data=None)\n",
    "df1['file_name'] = pd.Series(adjacent_file_name)\n",
    "df1['Condition'] = df1['file_name'].str.slice(0,2)\n",
    "df1['Participant'] = df1['file_name'].str.slice(6,9)\n",
    "df1['Mean Pitch distance'] = pd.Series(adjacent_distance_mean_pitch)\n",
    "df1['Min Pitch distance'] = pd.Series(adjacent_distance_min_pitch)\n",
    "df1['Max Pitch distance'] = pd.Series(adjacent_distance_max_pitch)\n",
    "df1['Mean Intensity distance'] = pd.Series(adjacent_distance_mean_intensity)\n",
    "df1['Min Intensity distance'] = pd.Series(adjacent_distance_min_intensity)\n",
    "df1['Max Intensity distance'] = pd.Series(adjacent_distance_max_intensity)\n",
    "df1['Jitter distance'] = pd.Series(adjacent_distance_jitter)\n",
    "df1['Shimmer distance'] = pd.Series(adjacent_distance_shimmer)\n",
    "df1['Mean Nhr distance'] = pd.Series(adjacent_distance_nhr)\n",
    "df1['Speechrate distance'] = pd.Series(adjacent_distance_speechrate)\n",
    "\n",
    "df1.to_csv(output_filename, index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 11:24:57 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0696dc604c884a37ad197b67a6ceeaf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-29 11:24:58 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2023-08-29 11:24:59 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2023-08-29 11:24:59 INFO: Using device: cpu\n",
      "2023-08-29 11:24:59 INFO: Loading: tokenize\n",
      "2023-08-29 11:24:59 INFO: Loading: pos\n",
      "2023-08-29 11:25:00 INFO: Loading: lemma\n",
      "2023-08-29 11:25:00 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#Eleventh step\n",
    "##The program initializes function to measure lexical and syntactic similarities using n-gram sequence methodology\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Name : Syntactic_and_Lexical_similarity_en.py\n",
    "# Author : Jay Kejriwal\n",
    "# Date   : 21-03-2022\n",
    "# Description : Program to measure lexical and syntactic similarity between two sentences\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "import stanza\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "def get_cosine(vec1, vec2):      #Function to measure cosine similarity\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text,ngram):  #Function to measure lexical similarities in sentence\n",
    "    n_grams = ngrams(nltk.word_tokenize(text), ngram)\n",
    "    new_sequence1 =  [ ' '.join(grams) for grams in n_grams]\n",
    "    return Counter(new_sequence1)\n",
    "\n",
    "\n",
    "def process_lex_text(text):         #Function to remove punctuation and extrat lemma and pos\n",
    "    words = re.sub(r'[.,\"\\-?:!;]', '', text.lower())\n",
    "    doc = nlp(words)\n",
    "    x=[f'{word.lemma}' for sent in doc.sentences for word in sent.words]\n",
    "    return ' '.join([str(elem) for elem in x])\n",
    "\n",
    "\n",
    "def process_syn_text(text):         #Function to remove punctuation and extrat lemma and pos\n",
    "    words = re.sub(r'[.,\"\\-?:!;]', '', text.lower())\n",
    "    doc = nlp(words)\n",
    "    l=[f'{word.upos}' for sent in doc.sentences for word in sent.words]\n",
    "    return ' '.join([str(elem) for elem in l])\n",
    "\n",
    "def calculate_lexical_similarity(text1, text2):\n",
    "    process1 = process_lex_text(text1)\n",
    "    process2 = process_lex_text(text2)\n",
    "    vector1 = text_to_vector(process1,ngram=1)\n",
    "    vector2 = text_to_vector(process2,ngram=1)\n",
    "    cosine = get_cosine(vector1, vector2)\n",
    "    return cosine\n",
    "\n",
    "# def calculate_syntactic_similarity(text1, text2):\n",
    "#     process1 = process_syn_text(text1)\n",
    "#     process2 = process_syn_text(text2)\n",
    "#     vector1 = text_to_vector(process1,ngram=2)\n",
    "#     vector2 = text_to_vector(process2,ngram=2)\n",
    "#     cosin = get_cosine(vector1, vector2)\n",
    "#     return cosin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\kejri/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\kejri\\AppData\\Local\\Temp\\ipykernel_14972\\3781440573.py:29: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  parser = stanford.StanfordParser(path_to_jar = jar_path, path_to_models_jar = models_jar_path, model_path = model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.935483870967742\n"
     ]
    }
   ],
   "source": [
    "#Twelveth step\n",
    "##The program initializes function to measure syntactic similarities on textual features using edit distance\n",
    "\n",
    "#Measure syntax similarity using edit distance\n",
    "# ------------------------------------------------------------------------\n",
    "# Name : Syntax_similarity.py\n",
    "# Author : Jay Kejriwal\n",
    "# Date   : 01-01-2022\n",
    "# Original__author__ = 'reihane'\n",
    "# Original__code__ = https://github.com/USC-CSSL/CASSIM\n",
    "# Description : Program re-written in Python 3.8.8 to measure syntax similarity between two sentences using Core-NLP\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment as su\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tree import ParentedTree\n",
    "from zss import simple_distance, Node\n",
    "from nltk.parse import stanford\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.data import load\n",
    "from collections import OrderedDict\n",
    "numnodes = 0\n",
    "\n",
    "#Specify path of dictionary\n",
    "sent_detector = load('file:D:\\Jay\\Lexical_Syntactic\\stanford\\english.pickle')\n",
    "#Specify path of jar files\n",
    "jar_path = r'D:\\Jay\\Lexical_Syntactic\\stanford\\stanford-corenlp-4.5.0\\stanford-corenlp-4.5.0.jar'\n",
    "models_jar_path = r'D:\\Jay\\Lexical_Syntactic\\stanford\\stanford-corenlp-4.5.0\\stanford-corenlp-4.5.0-models.jar'\n",
    "#Specify path of model\n",
    "model_path= r'D:\\Jay\\Lexical_Syntactic\\stanford\\englishPCFG.ser.gz'\n",
    "parser = stanford.StanfordParser(path_to_jar = jar_path, path_to_models_jar = models_jar_path, model_path = model_path)\n",
    "\n",
    "def convert_mytree(nltktree,pnode):\n",
    "    global numnodes\n",
    "    for node in nltktree:\n",
    "        numnodes+=1\n",
    "        if type(node) is nltk.ParentedTree:\n",
    "            tempnode = Node(node.label())\n",
    "            pnode.addkid(tempnode)\n",
    "            convert_mytree(node,tempnode)\n",
    "    return pnode\n",
    "\n",
    "def calculate_syntactic_similarity(doc1, doc2, average=True): #syntax similarity of two single documents\n",
    "    global numnodes\n",
    "    doc1sents = sent_detector.tokenize(doc1.strip())\n",
    "    doc2sents = sent_detector.tokenize(doc2.strip())\n",
    "    for s in doc1sents: # to handle unusual long sentences.\n",
    "        if len(s.split())>100:\n",
    "            return \"NA\"\n",
    "    for s in doc2sents:\n",
    "        if len(s.split())>100:\n",
    "            return \"NA\"\n",
    "    try: #to handle parse errors. Parser errors might happen in cases where there is an unsuall long word in the sentence.\n",
    "        doc1parsed = parser.raw_parse_sents((doc1sents))\n",
    "        doc2parsed = parser.raw_parse_sents((doc2sents))\n",
    "    except Exception as e:\n",
    "        sys.stderr.write(str(e))\n",
    "        return \"NA\"\n",
    "    costMatrix = []\n",
    "    doc1parsed = list(doc1parsed)\n",
    "    for i in range(len(doc1parsed)):\n",
    "        doc1parsed[i] = list(doc1parsed[i])[0]\n",
    "    doc2parsed = list(doc2parsed)\n",
    "    for i in range(len(doc2parsed)):\n",
    "        doc2parsed[i] = list(doc2parsed[i])[0]\n",
    "    for i in range(len(doc1parsed)):\n",
    "        numnodes = 0\n",
    "        sentencedoc1 = ParentedTree.convert(doc1parsed[i])\n",
    "        tempnode = Node(sentencedoc1.root().label())\n",
    "        new_sentencedoc1 = convert_mytree(sentencedoc1,tempnode)\n",
    "        temp_costMatrix = []\n",
    "        sen1nodes = numnodes\n",
    "        for j in range(len(doc2parsed)):\n",
    "            numnodes=0.0\n",
    "            sentencedoc2 = ParentedTree.convert(doc2parsed[j])\n",
    "            tempnode = Node(sentencedoc2.root().label())\n",
    "            new_sentencedoc2 = convert_mytree(sentencedoc2,tempnode)\n",
    "            ED = simple_distance(new_sentencedoc1, new_sentencedoc2)\n",
    "            ED = ED / (numnodes + sen1nodes)\n",
    "            temp_costMatrix.append(ED)\n",
    "        costMatrix.append(temp_costMatrix)\n",
    "    costMatrix = np.array(costMatrix)\n",
    "    if average==True:\n",
    "        return 1-np.mean(costMatrix)\n",
    "    else:\n",
    "        indexes = su(costMatrix)\n",
    "        total = 0\n",
    "        rowMarked = [0] * len(doc1parsed)\n",
    "        colMarked = [0] * len(doc2parsed)\n",
    "        for row, column in indexes:\n",
    "            total += costMatrix[row][column]\n",
    "            rowMarked[row] = 1\n",
    "            colMarked [column] = 1\n",
    "        for k in range(len(rowMarked)):\n",
    "            if rowMarked[k]==0:\n",
    "                total+= np.min(costMatrix[k])\n",
    "        for c in range(len(colMarked)):\n",
    "            if colMarked[c]==0:\n",
    "                total+= np.min(costMatrix[:,c])\n",
    "        maxlengraph = max(len(doc1parsed),len(doc2parsed))\n",
    "        return 1-(total/maxlengraph)\n",
    "\n",
    "print(calculate_syntactic_similarity(\"Her face lit up.\",\"Her candle lit up itself.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thirteenth Step\n",
    "## Program to measure entrainment distance using textual, semantic and auditory embeddings\n",
    "## The program reads pkl file generated in the Fifth step and measures adjacent and self scores at lexical, syntactic, semantic and acoustic levels\n",
    "## Only adjacent scores were used for analysis\n",
    "## The program generates a text file for each session with the following column information\n",
    "###Adjacent score on each linguistic level\n",
    "# Same_pair_lexical \tSame_pair_syntactic\tSame_pair_semantic\tSame_pair_audio\t\n",
    "###Self score of each speaker at each linguistic level\n",
    "#Speaker1_lexical_self_distance\tSpeaker2_lexical_self_distance\t\n",
    "#Speaker1_syntactic_self_distance\tSpeaker2_syntactic_self_distance\t\n",
    "#Speaker1_semantic_self_distance\tSpeaker2_semantic_self_distance\t\n",
    "#Speaker1_audio_self_distance\tSpeaker2_audio_self_distance\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import util\n",
    "import random\n",
    "import subprocess\n",
    "import re\n",
    "import glob\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "#Define path of embedding files and output\n",
    "audio_embedding_path = r'D:\\Jay\\GAZE\\Pickle\\audio'\n",
    "text_embedding_path = r'D:\\Jay\\GAZE\\Pickle\\text'\n",
    "semantic_embedding_path = r'D:\\Jay\\GAZE\\Pickle\\semantic'\n",
    "\n",
    "output_path = r'D:\\Jay\\GAZE\\output'\n",
    "\n",
    "for audiofile,textfile,semanticfile in zip(os.listdir(audio_embedding_path),os.listdir(text_embedding_path),os.listdir(semantic_embedding_path)):\n",
    "        if audiofile.endswith(\".pkl\") & textfile.endswith(\".pkl\") & semanticfile.endswith(\".pkl\"):\n",
    "            current_audiofile = os.path.join(audio_embedding_path,audiofile)\n",
    "            current_textfile = os.path.join(text_embedding_path,textfile)\n",
    "            current_semanticfile = os.path.join(semantic_embedding_path,semanticfile)\n",
    "\n",
    "            output_filename= os.path.splitext(os.path.join(output_path, os.path.basename(audiofile)))[0]\n",
    "            #Open audio, text and semantic pickle files\n",
    "            with open(current_audiofile, 'rb') as a, open(current_textfile, 'rb') as b, open(current_semanticfile, 'rb') as c:\n",
    "                audio_embeddings_tensor=pickle.load(a)\n",
    "                sentence_embeddings_tensor=pickle.load(b)\n",
    "                semantic_embeddings_tensor=pickle.load(c)\n",
    "                \n",
    "                lexical_speaker1=[]\n",
    "                lexical_speaker2=[]\n",
    "                syntactic_speaker1=[]\n",
    "                syntactic_speaker2=[]\n",
    "                semantic_speaker1=[]\n",
    "                semantic_speaker2=[]\n",
    "                audio_speaker1=[]\n",
    "                audio_speaker2=[]\n",
    "                \n",
    "                same_pair_lexical = []\n",
    "                same_pairs_syntactic = []\n",
    "                same_pairs_semantic = []\n",
    "                same_pairs_audio = []\n",
    "                \n",
    "                for i in range(0, len(sentence_embeddings_tensor)-1):\n",
    "\n",
    "                    if(i%2==0):\n",
    "                        #Measure Partner distance for even turns\n",
    "                        spl=calculate_lexical_similarity(sentence_embeddings_tensor[i],sentence_embeddings_tensor[i+1])\n",
    "                        same_pair_lexical.append(spl)\n",
    "                        spsy=calculate_syntactic_similarity(sentence_embeddings_tensor[i],sentence_embeddings_tensor[i+1])\n",
    "                        same_pairs_syntactic.append(spsy)\n",
    "                        sps=util.cos_sim(semantic_embeddings_tensor[i],semantic_embeddings_tensor[i+1]).item()\n",
    "                        same_pairs_semantic.append(sps)\n",
    "                        spa=util.cos_sim(audio_embeddings_tensor[i],audio_embeddings_tensor[i+1]).item()\n",
    "                        same_pairs_audio.append(spa)\n",
    "                        \n",
    "                    elif(i%2!=0):\n",
    "                        #Measure Partner distance for odd turns\n",
    "                        spl=calculate_lexical_similarity(sentence_embeddings_tensor[i],sentence_embeddings_tensor[i+1])\n",
    "                        same_pair_lexical.append(spl)\n",
    "                        spsy=calculate_syntactic_similarity(sentence_embeddings_tensor[i],sentence_embeddings_tensor[i+1])\n",
    "                        same_pairs_syntactic.append(spsy)\n",
    "                        sps=util.cos_sim(semantic_embeddings_tensor[i],semantic_embeddings_tensor[i+1]).item()\n",
    "                        same_pairs_semantic.append(sps)\n",
    "                        spa=util.cos_sim(audio_embeddings_tensor[i],audio_embeddings_tensor[i+1]).item()\n",
    "                        same_pairs_audio.append(spa)\n",
    "\n",
    "                for n in range(0, len(sentence_embeddings_tensor)-2):\n",
    "                    #Measure self-distance\n",
    "                    if(n%2==0):\n",
    "                        same_spkr1_lexical=calculate_lexical_similarity(sentence_embeddings_tensor[n],sentence_embeddings_tensor[n+2])\n",
    "                        lexical_speaker1.append(same_spkr1_lexical)\n",
    "                        same_spkr1_syntactic=calculate_syntactic_similarity(sentence_embeddings_tensor[n],sentence_embeddings_tensor[n+2])\n",
    "                        syntactic_speaker1.append(same_spkr1_syntactic)\n",
    "                        same_spkr1_semantic=util.cos_sim(semantic_embeddings_tensor[n],semantic_embeddings_tensor[n+2]).item()\n",
    "                        semantic_speaker1.append(same_spkr1_semantic)\n",
    "                        same_spkr1_audio=util.cos_sim(audio_embeddings_tensor[n],audio_embeddings_tensor[n+2]).item()\n",
    "                        audio_speaker1.append(same_spkr1_audio)\n",
    "                \n",
    "                    elif(n%2!=0):\n",
    "                        same_spkr2_lexical=calculate_lexical_similarity(sentence_embeddings_tensor[n],sentence_embeddings_tensor[n+2])\n",
    "                        lexical_speaker2.append(same_spkr2_lexical)\n",
    "                        same_spkr2_syntactic=calculate_syntactic_similarity(sentence_embeddings_tensor[n],sentence_embeddings_tensor[n+2])\n",
    "                        syntactic_speaker2.append(same_spkr2_syntactic)\n",
    "                        same_spkr2_semantic=util.cos_sim(semantic_embeddings_tensor[n],semantic_embeddings_tensor[n+2]).item()\n",
    "                        semantic_speaker2.append(same_spkr2_semantic)\n",
    "                        same_spkr2_audio=util.cos_sim(audio_embeddings_tensor[n],audio_embeddings_tensor[n+2]).item()\n",
    "                        audio_speaker2.append(same_spkr2_audio)\n",
    "                        \n",
    "                df = pd.DataFrame(data=None)\n",
    "                df['Same_pair_lexical'] = pd.Series(same_pair_lexical)\n",
    "                df['Same_pair_syntactic'] = pd.Series(same_pairs_syntactic)\n",
    "                df['Same_pair_semantic'] = pd.Series(same_pairs_semantic)\n",
    "                df['Same_pair_audio'] = pd.Series(same_pairs_audio)\n",
    "\n",
    "                df['Speaker1_lexical_self_distance'] = pd.Series(lexical_speaker1)\n",
    "                df['Speaker2_lexical_self_distance'] = pd.Series(lexical_speaker2)\n",
    "                df['Speaker1_syntactic_self_distance'] = pd.Series(syntactic_speaker1)\n",
    "                df['Speaker2_syntactic_self_distance'] = pd.Series(syntactic_speaker2)\n",
    "                df['Speaker1_semantic_self_distance'] = pd.Series(semantic_speaker1)\n",
    "                df['Speaker2_semantic_self_distance'] = pd.Series(semantic_speaker2)\n",
    "                df['Speaker1_audio_self_distance'] = pd.Series(audio_speaker1)\n",
    "                df['Speaker2_audio_self_distance'] = pd.Series(audio_speaker2)\n",
    "\n",
    "                df.to_csv(output_filename, index=False,sep='\\t')\n",
    "                \n",
    "                lexical_speaker1=[]\n",
    "                lexical_speaker2=[]\n",
    "                syntactic_speaker1=[]\n",
    "                syntactic_speaker2=[]\n",
    "                semantic_speaker1=[]\n",
    "                semantic_speaker2=[]\n",
    "                audio_speaker1=[]\n",
    "                audio_speaker2=[]\n",
    "                same_pair_lexical = []\n",
    "                same_pairs_syntactic = []\n",
    "                same_pairs_semantic = []\n",
    "                same_pairs_audio = []\n",
    "                del df,spl,spsy,sps,spa,same_spkr1_lexical,\n",
    "                same_spkr1_semantic,same_spkr1_syntactic,same_spkr1_audio,same_spkr2_lexical,\n",
    "                same_spkr2_semantic,same_spkr2_syntactic,same_spkr2_audio\n",
    "\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final step \n",
    "## Program to merge output files generated in previous\n",
    "# The program concatenates adjacent and self-score obtained of each inidividual sessions into one file\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "#Specify path for files generated in previous step\n",
    "list_of_files = glob.glob(r'D:\\Jay\\GAZE\\output\\*',recursive=True)\n",
    "\n",
    "#Specify output path\n",
    "out_name = r'D:\\Jay\\GAZE\\merged_stats'\n",
    "li = []\n",
    "\n",
    "for filename in list_of_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0, delimiter='\\t')\n",
    "    basename = os.path.basename(filename)\n",
    "    file_name = os.path.splitext(basename)[0]\n",
    "    df['File_name'] = file_name\n",
    "    df['Condition'] = df['File_name'].str.slice(0,2)\n",
    "    df['Participant'] = df['File_name'].str.slice(6,9)\n",
    "    #Re-arrange columns with filename as first column\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    #Define new arranged column to data-frame\n",
    "    df = df[cols]\n",
    "    #Append the dataframe to list\n",
    "    li.append(df)\n",
    "frame = pd.concat(li, axis=0, ignore_index=True) \n",
    "\n",
    "frame.to_csv(out_name+'.csv', index=False,sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze the data using python or JASP.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "#Specify input file\n",
    "file_name=r'D:\\Jay\\GAZE\\merged_stats.csv'\n",
    "data = pd.read_csv(file_name,delimiter='\\t')\n",
    "data.head()\n",
    "data.rename(columns = {'Mean Pitch distance':'Mean_Pitch_distance'}, inplace = True)\n",
    "md = smf.ols(formula=\"Mean_Pitch_distance ~  Condition\", data=data, groups=data[\"Participant\"])\n",
    "mdf = md.fit()\n",
    "print(mdf.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
